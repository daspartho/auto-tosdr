{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n3gMtSAsBohf"
      },
      "outputs": [],
      "source": [
        "!pip install transformers sentencepiece datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, ClassLabel\n",
        "\n",
        "ds = load_dataset(\"csv\",data_files=\"tos.csv\")[\"train\"]\n",
        "\n",
        "label_names = ds.unique('label') # list of labels (subreddits)\n",
        "labels = ClassLabel(\n",
        "    num_classes=len(label_names), \n",
        "    names=label_names,\n",
        "    )\n",
        "\n",
        "ds = ds.train_test_split(test_size=0.2, shuffle=True)\n",
        "ds"
      ],
      "metadata": {
        "id": "be5lAEi0CckN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "model_name = 'distilbert-base-uncased'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "def tok_func(x): \n",
        "    tok_x = tokenizer(x[\"text\"], padding=True, truncation=True)\n",
        "    tok_x['label'] = labels.str2int(x['label'])\n",
        "    return tok_x\n",
        "tok_ds = ds.map(tok_func, batched=True)"
      ],
      "metadata": {
        "id": "12cyfW9QCeOZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_metric\n",
        "import numpy as np\n",
        "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "\n",
        "bs = 32\n",
        "epochs = 10\n",
        "lr = 1e-5\n",
        "\n",
        "metric = load_metric(\"accuracy\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "args = TrainingArguments('model', learning_rate=lr, warmup_ratio=0.1, lr_scheduler_type='cosine', fp16=True, \n",
        "                         evaluation_strategy=\"epoch\", per_device_train_batch_size=bs, per_device_eval_batch_size=bs*2, \n",
        "                         num_train_epochs=epochs, weight_decay=0.01, report_to='none')\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(label_names))\n",
        "\n",
        "trainer = Trainer(model, args, train_dataset=tok_ds['train'], eval_dataset=tok_ds['test'], \n",
        "                  tokenizer=tokenizer, compute_metrics=compute_metrics)\n",
        "\n",
        "trainer.train();"
      ],
      "metadata": {
        "id": "S54ljJ5qCf-R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TextClassificationPipeline\n",
        "\n",
        "pipe = TextClassificationPipeline(model=model, tokenizer=tokenizer, device=0)\n",
        "\n",
        "text = \"We reserve the right to make changes to our site, policies, Service Terms, and these Conditions of Use at any time\"\n",
        "label_names[int(pipe(text)[0]['label'].split('_')[-1])]"
      ],
      "metadata": {
        "id": "TmZKbTVvuAel"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}